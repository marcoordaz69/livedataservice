# Trading Data Service - Cloud Deployment Package

This is the standalone live data service that captures real-time market data and validates trade setups. It connects to your Supabase database and runs independently from your main trading application.

## ğŸ¯ What This Service Does

- **Live Market Data**: Captures real-time NQ futures data from Databento
- **Trade Validation**: Monitors trade setups and triggers entries/exits
- **Database Updates**: Stores OHLCV data and trade events in Supabase
- **24/7 Operation**: Designed to run continuously in the cloud

## ğŸ“¦ Package Contents

```
trading-data-service/
â”œâ”€â”€ price_capture/               # Live data components
â”‚   â”œâ”€â”€ live_data_launcher.py   # Main orchestrator
â”‚   â”œâ”€â”€ simple_ohlcv.py         # Live data streaming
â”‚   â”œâ”€â”€ setup_monitor.py        # Trade validation logic
â”‚   â”œâ”€â”€ live_data_service.py    # Historical backfill
â”‚   â””â”€â”€ monitor_setup_validator.py  # Real-time monitoring
â”œâ”€â”€ database/                    # Database utilities
â”‚   â”œâ”€â”€ trading_db.py           # Database operations
â”‚   â”œâ”€â”€ db.py                   # Connection management
â”‚   â”œâ”€â”€ utils.py                # Helper functions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ config.example             # Environment variables template
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Deployment

### Option 1: Railway (Recommended)
1. Push this folder to a Git repository
2. Connect Railway to your repository
3. Set environment variables (see below)
4. Deploy as a worker service

### Option 2: Render
1. Connect your repository to Render
2. Create a new Background Worker
3. Set build command: `pip install -r requirements.txt`
4. Set start command: `python price_capture/live_data_launcher.py`

### Option 3: VPS Deployment
```bash
# Install dependencies
pip install -r requirements.txt

# Create systemd service
sudo cp live-data.service /etc/systemd/system/
sudo systemctl enable live-data
sudo systemctl start live-data
```

## âš™ï¸ Environment Variables

Required environment variables (copy from config.example):

```bash
# Databento API Key
DATABENTO_API_KEY=your_api_key_here

# Supabase Database Connection
host=your_supabase_host
user=your_db_user
password=your_db_password
port=5432
dbname=postgres
```

## ğŸ”§ Local Testing

Before deploying, test the service locally:

```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
cp config.example .env
# Edit .env with your actual values

# Test the service
python price_capture/live_data_launcher.py --live-only
```

## ğŸ“Š Monitoring

The service provides logs for monitoring:
- Market data ingestion status
- Trade setup validations
- Database connection health
- Error reporting

## ğŸ”„ Integration with Main App

This service communicates with your main trading application through the shared Supabase database:

- **Writes**: Live OHLCV data, trade status updates
- **Reads**: Trade setups created by your main app
- **No direct API calls** between services

## ğŸ’° Cost Estimates

- **Railway**: $5-10/month
- **Render**: $7/month  
- **VPS**: $3-5/month

## ğŸ› ï¸ Troubleshooting

Common issues:
1. **Database connection**: Check Supabase credentials
2. **API limits**: Verify Databento API key and quota
3. **Missing data**: Check market hours and symbol availability
4. **Import errors**: Ensure all dependencies are installed

## ğŸ“‹ Service Commands

```bash
# Full service (backfill + live data + monitoring)
python price_capture/live_data_launcher.py

# Live data only
python price_capture/live_data_launcher.py --live-only

# Backfill only
python price_capture/live_data_launcher.py --backfill-only --days 7

# Disable monitoring
python price_capture/live_data_launcher.py --no-monitor
```

Your main trading application continues to run locally while this service handles 24/7 data capture in the cloud.